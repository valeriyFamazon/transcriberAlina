import asyncio
import base64
import json
import logging
import os
import tempfile
import time
from typing import Dict, Any
import ctypes
import wave
import contextlib
try:
    import webrtcvad  # type: ignore
    HAS_WEBRTCVAD = True
except Exception:
    webrtcvad = None  # type: ignore
    HAS_WEBRTCVAD = False
from collections import deque
import numpy as np
from scipy.signal import butter, lfilter
import soundfile as sf
from transformers import pipeline
import openai
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydub import AudioSegment
import uvicorn
import dotenv

dotenv.load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å —ç–º–æ–¥–∑–∏
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è OpenAI (–∫–ª—é—á –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è)
openai.api_key = os.getenv("OPENAI_API_KEY", "")
if not openai.api_key:
    logger.warning("‚ö†Ô∏è OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏")

app = FastAPI()

# CORS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:5174"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ FFmpeg
transcriber_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
ffmpeg_path = os.path.join(transcriber_dir, "ffmpeg.exe")
ffprobe_path = os.path.join(transcriber_dir, "ffprobe.exe")

logger.info(f"üîç –ò—â–µ–º FFmpeg –≤: {ffmpeg_path}")
logger.info(f"üîç –ò—â–µ–º FFprobe –≤: {ffprobe_path}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
ffmpeg_exists = os.path.exists(ffmpeg_path)
ffprobe_exists = os.path.exists(ffprobe_path)

if ffmpeg_exists:
    AudioSegment.converter = ffmpeg_path
    logger.info("‚úÖ FFmpeg –Ω–∞–π–¥–µ–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
else:
    logger.warning("‚ö†Ô∏è FFmpeg –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–∞–ø–∫–µ transcriber")

if ffprobe_exists:
    AudioSegment.ffprobe = ffprobe_path
    logger.info("‚úÖ FFprobe –Ω–∞–π–¥–µ–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
else:
    logger.warning("‚ö†Ô∏è FFprobe –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–∞–ø–∫–µ transcriber")

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è pydub
try:
    from pydub.utils import which
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ pydub –º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ ffmpeg
    if not which("ffmpeg"):
        logger.info("üîß –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º pydub –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ ffmpeg")
        import pydub
        pydub.AudioSegment.converter = ffmpeg_path
        pydub.AudioSegment.ffprobe = ffprobe_path
        logger.info("‚úÖ Pydub –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ ffmpeg")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å pydub: {e}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å FFmpeg
def test_ffmpeg():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å FFmpeg"""
    try:
        if ffmpeg_exists:
            import subprocess
            result = subprocess.run([ffmpeg_path, "-version"], 
                                 capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                logger.info("‚úÖ FFmpeg —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
                return True
            else:
                logger.error(f"‚ùå FFmpeg –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {result.stderr}")
                return False
        else:
            logger.error("‚ùå FFmpeg –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è FFmpeg: {e}")
        return False

# –¢–µ—Å—Ç–∏—Ä—É–µ–º FFmpeg –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
ffmpeg_working = test_ffmpeg()

"""
–ü—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º: –ø–æ–ª—É—á–µ–Ω–∏–µ base64-–∞—É–¥–∏–æ —á–∞–Ω–∫–æ–≤ –ø–æ WS, –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ WAV 16kHz mono,
–æ—Ç–ø—Ä–∞–≤–∫–∞ –≤ OpenAI Whisper –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ç–µ–∫—Å—Ç–∞. –ë–µ–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏.
"""

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤ –∫–∞–∫ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–±–µ–∑ —Å–∫–ª–µ–π–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤/–∫–ª–∞—Å—Ç–µ—Ä–æ–≤)
MIN_CHUNK_BYTES = 2 * 1024  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è–º
connection_engine: Dict[int, str] = {}

DEFAULT_OPENAI_PROMPT = (
    "–ó–∞–¥–∞—á–∞: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ä–µ—á—å —Å–ø–∏–∫–µ—Ä–∞ –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ –∞—É–¥–∏–æ. "
    "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: 1) –ò–≥–Ω–æ—Ä–∏—Ä—É–π –ª—é–±—ã–µ –≤—Å—Ç–∞–≤–∫–∏, —Å—É–±—Ç–∏—Ç—Ä—ã, –∑–∞—Å—Ç–∞–≤–∫–∏ –∏ —Ñ—Ä–∞–∑—ã –≤—Ä–æ–¥–µ '–°—É–±—Ç–∏—Ç—Ä—ã –¥–µ–ª–∞–ª DimaTorzok', '–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç', '[–º—É–∑—ã–∫–∞]', '[–∞–ø–ª–æ–¥–∏—Å–º–µ–Ω—Ç—ã]'. "
    "2) –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è –∏ –Ω–µ –¥–æ–ø–æ–ª–Ω—è–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. 3) –°–æ—Ö—Ä–∞–Ω—è–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ—á–∏. "
    "4) –°–æ—Ö—Ä–∞–Ω—è–π —è–∑—ã–∫ –∏—Å—Ö–æ–¥–Ω–æ–π —Ä–µ—á–∏ (–æ—Å–Ω–æ–≤–Ω–æ–π: —Ä—É—Å—Å–∫–∏–π). 5) –£–±–∏—Ä–∞–π —ç—ç—ç/–º–º–º/–º–µ–∂–¥–æ–º–µ—Ç–∏—è –∏ —à—É–º–æ–≤—ã–µ —Å–ª–æ–≤–∞."
)

# –õ–æ–∫–∞–ª—å–Ω–∞—è HF-–ø–∞–π–ø–ª–∞–π–Ω (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
_local_asr = None

def get_local_asr():
    global _local_asr
    if _local_asr is None:
        model_name = os.getenv('HF_LOCAL_ASR_MODEL', 'openai/whisper-small')
        logger.info(f"üß† –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é ASR –º–æ–¥–µ–ª—å: {model_name}")
        _local_asr = pipeline(
            task='automatic-speech-recognition',
            model=model_name,
            device=-1,
        )
    return _local_asr


def map_language_for_whisper(language_code: str) -> str:
    code = (language_code or '').lower()
    mapping = {
        'ru': 'russian',
        'en': 'english',
        'uk': 'ukrainian',
        'ua': 'ukrainian',
    }
    return mapping.get(code, code or 'russian')

def to_short_path(path: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç 8.3 short path –Ω–∞ Windows –¥–ª—è ASCII-–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (ffmpeg), –∏–Ω–∞—á–µ –∏—Å—Ö–æ–¥–Ω—ã–π –ø—É—Ç—å."""
    try:
        if os.name != 'nt':
            return path
        GetShortPathNameW = ctypes.windll.kernel32.GetShortPathNameW  # type: ignore[attr-defined]
        GetShortPathNameW.argtypes = [ctypes.c_wchar_p, ctypes.c_wchar_p, ctypes.c_uint]
        GetShortPathNameW.restype = ctypes.c_uint
        buffer = ctypes.create_unicode_buffer(260)
        result = GetShortPathNameW(path, buffer, 260)
        return buffer.value if result else path
    except Exception:
        return path


def butter_highpass(cutoff_hz: float, sample_rate: int, order: int = 2):
    nyq = 0.5 * sample_rate
    normal_cutoff = cutoff_hz / nyq
    b, a = butter(order, normal_cutoff, btype='high', analog=False)
    return b, a


def apply_highpass(signal: np.ndarray, sample_rate: int, cutoff_hz: float = 80.0) -> np.ndarray:
    if signal.size == 0:
        return signal
    b, a = butter_highpass(cutoff_hz, sample_rate)
    return lfilter(b, a, signal)


def read_wave(path: str):
    with contextlib.closing(wave.open(path, 'rb')) as wf:
        num_channels = wf.getnchannels()
        assert num_channels == 1
        sample_width = wf.getsampwidth()
        assert sample_width == 2
        sample_rate = wf.getframerate()
        assert sample_rate in (8000, 16000, 32000, 48000)
        pcm_data = wf.readframes(wf.getnframes())
        return pcm_data, sample_rate


def write_wave(path: str, audio: bytes, sample_rate: int):
    with contextlib.closing(wave.open(path, 'wb')) as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio)


class Frame:
    def __init__(self, bytes_data: bytes, timestamp: float, duration: float):
        self.bytes = bytes_data
        self.timestamp = timestamp
        self.duration = duration


def frame_generator(frame_duration_ms: int, audio: bytes, sample_rate: int):
    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)
    offset = 0
    timestamp = 0.0
    duration = (float(n) / (2 * sample_rate))
    while offset + n <= len(audio):
        yield Frame(audio[offset:offset + n], timestamp, duration)
        timestamp += duration
        offset += n


def energy_based_vad(audio: bytes, sample_rate: int, frame_ms: int = 30, threshold_db: float = -45.0) -> bytes:
    if not audio:
        return audio
    samples = np.frombuffer(audio, dtype=np.int16).astype(np.float32)
    frame_len = int(sample_rate * frame_ms / 1000)
    if frame_len <= 0:
        return audio
    kept = []
    pad_frames = 3
    recent_voice = 0
    for i in range(0, len(samples), frame_len):
        frame = samples[i:i+frame_len]
        if frame.size == 0:
            continue
        rms = np.sqrt(np.mean(np.square(frame))) + 1e-8
        db = 20 * np.log10(rms / 32768.0 + 1e-12)
        is_voiced = db > threshold_db
        if is_voiced:
            recent_voice = pad_frames
        if is_voiced or recent_voice > 0:
            kept.append(frame)
        if recent_voice > 0 and not is_voiced:
            recent_voice -= 1
    if not kept:
        return audio
    out = np.concatenate(kept).astype(np.int16).tobytes()
    return out


def vad_collect(audio: bytes, sample_rate: int, aggressiveness: int = 2) -> bytes:
    if HAS_WEBRTCVAD:
        vad = webrtcvad.Vad(aggressiveness)
        frames = list(frame_generator(30, audio, sample_rate))
        ring_buffer = deque(maxlen=int(300 / 30))
        triggered = False
        voiced: list[bytes] = []
        for frame in frames:
            is_speech = vad.is_speech(frame.bytes, sample_rate)
            if not triggered:
                ring_buffer.append((frame, is_speech))
                num_voiced = len([f for f, speech in ring_buffer if speech])
                if num_voiced > 0.9 * ring_buffer.maxlen:
                    triggered = True
                    voiced.extend(f.bytes for f, _ in ring_buffer)
                    ring_buffer.clear()
            else:
                voiced.append(frame.bytes)
                ring_buffer.append((frame, is_speech))
                num_unvoiced = len([f for f, speech in ring_buffer if not speech])
                if num_unvoiced > 0.9 * ring_buffer.maxlen:
                    triggered = False
                    ring_buffer.clear()
        return b''.join(voiced) if voiced else b''
    else:
        logger.info("‚ÑπÔ∏è webrtcvad –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–Ω–µ—Ä–≥–æ-VAD")
        return energy_based_vad(audio, sample_rate)


def preprocess_wav(input_wav: str) -> str:
    pcm, sr = read_wave(input_wav)
    # high-pass
    samples = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0
    samples = apply_highpass(samples, sr, 80.0)
    samples = np.clip(samples, -1.0, 1.0)
    hp_bytes = (samples * 32767.0).astype(np.int16).tobytes()
    # VAD
    voiced = vad_collect(hp_bytes, sr, aggressiveness=2)
    if not voiced or len(voiced) < 320:  # <10 ms
        return input_wav
    out_path = input_wav.replace('.wav', '_vad.wav')
    write_wave(out_path, voiced, sr)
    return out_path


def clean_transcript_text(text: str) -> str:
    if not text:
        return text
    deny = [
        '—Å—É–±—Ç–∏—Ç—Ä—ã –¥–µ–ª–∞–ª dimatorzok',
        '–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç',
        '–ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å',
        '–ª–∞–π–∫',
        '–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å',
        '—Å–ø–∞—Å–∏–±–æ –∑–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä'
    ]
    t = text.strip()
    low = t.lower()
    for phrase in deny:
        low = low.replace(phrase, ' ')
    # —É–±–µ—Ä—ë–º –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ –≤—Å—Ç–∞–≤–∫–∏ —Ç–∏–ø–∞ [–º—É–∑—ã–∫–∞], [–∞–ø–ª–æ–¥–∏—Å–º–µ–Ω—Ç—ã]
    import re
    low = re.sub(r"\[.*?\]", " ", low)
    # —É–±–µ—Ä—ë–º –º–Ω–æ–≥–æ—Ç–æ—á–∏—è-–∑–∞–≥–ª—É—à–∫–∏
    low = re.sub(r"(^|\s)(–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ\s+—Å–ª–µ–¥—É–µ—Ç\.*)$", " ", low)
    cleaned = ' '.join(low.split())
    return cleaned

def guess_extension(audio_bytes: bytes, mime: str) -> str:
    """–ì—Ä—É–±–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø–æ —Å–∏–≥–Ω–∞—Ç—É—Ä–µ –∏ MIME."""
    if audio_bytes.startswith(b'RIFF') and b'WAVE' in audio_bytes[:20]:
        return '.wav'
    if audio_bytes.startswith(b'\x1a\x45\xdf\xa3'):
        return '.webm'
    if audio_bytes.startswith(b'ftyp'):
        return '.mp4'
    if audio_bytes.startswith(b'OggS'):
        return '.ogg'
    m = (mime or '').lower()
    if 'webm' in m:
        return '.webm'
    if 'ogg' in m:
        return '.ogg'
    if 'mp4' in m or 'm4a' in m:
        return '.mp4'
    if 'wav' in m:
        return '.wav'
    return '.wav'

async def transcribe_audio_chunk(audio_data: bytes, audio_format: str, language: str = "ru", engine: str = "openai", prompt: str = None) -> Dict[str, Any]:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞—É–¥–∏–æ —á–∞–Ω–∫ –≤ OpenAI –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏"""
    temp_file = None
    try:
        logger.info(f"üéôÔ∏è === –ù–ê–ß–ê–õ–û –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–ò ===")
        logger.info(f"üéôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —á–∞–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–º {len(audio_data)} –±–∞–π—Ç")

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
        if len(audio_data) < 1000:
            logger.warning("‚ö†Ô∏è –ß–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (< 1KB)")
            return {"error": "–ß–∞–Ω–∫ –∞—É–¥–∏–æ —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è"}
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É temp –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        temp_dir = os.path.join(transcriber_dir, "temp")
        os.makedirs(temp_dir, exist_ok=True)
        logger.info(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {temp_dir}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–æ –ø–µ—Ä–≤—ã–º –±–∞–π—Ç–∞–º
        logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö –±–∞–π—Ç: {list(audio_data[:10])}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–æ –ø–µ—Ä–≤—ã–º –±–∞–π—Ç–∞–º
        if audio_data.startswith(b'RIFF') and b'WAVE' in audio_data[:20]:
            logger.info("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω WAV —Ñ–æ—Ä–º–∞—Ç")
            file_extension = '.wav'
        elif audio_data.startswith(b'\x1a\x45\xdf\xa3'):
            logger.info("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω WebM/MKV —Ñ–æ—Ä–º–∞—Ç")
            file_extension = '.webm'
        elif audio_data.startswith(b'ftyp'):
            logger.info("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω MP4 —Ñ–æ—Ä–º–∞—Ç")
            file_extension = '.mp4'
        elif audio_data.startswith(b'OggS'):
            logger.info("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω OGG —Ñ–æ—Ä–º–∞—Ç")
            file_extension = '.ogg'
        else:
            logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, –ø–µ—Ä–≤—ã–µ –±–∞–π—Ç—ã: {list(audio_data[:10])}")
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø–µ—Ä–µ—Ç—å—Å—è –Ω–∞ MIME –∏–∑ –∫–ª–∏–µ–Ω—Ç–∞
            fmt_lower = (audio_format or '').lower()
            if 'webm' in fmt_lower:
                logger.info("üéµ –û–ø—Ä–µ–¥–µ–ª–µ–Ω –∫–∞–∫ WebM —Å Opus –∫–æ–¥–µ–∫–æ–º")
                file_extension = '.webm'
            elif 'ogg' in fmt_lower:
                file_extension = '.ogg'
            elif 'mp4' in fmt_lower or 'm4a' in fmt_lower:
                file_extension = '.mp4'
            elif 'wav' in fmt_lower:
                file_extension = '.wav'
            else:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±—É–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ WAV –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏")
                file_extension = '.wav'
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º (ASCII-–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π –ø—É—Ç—å)
        temp_file = os.path.join(temp_dir, f"audio_{int(time.time() * 1000)}{file_extension}")
        logger.info(f"üìÅ –°–æ–∑–¥–∞–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è OpenAI: {temp_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª
        with open(temp_file, 'wb') as f:
            f.write(audio_data)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–∏–ª–∏ {len(audio_data)} –±–∞–π—Ç –≤ {temp_file}")
        safe_temp_file = to_short_path(temp_file)
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ FFmpeg CLI ‚Üí WAV 16kHz mono
            logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –∏–∑: {temp_file}")
            output_file = os.path.join(temp_dir, f"openai_audio_{int(time.time() * 1000)}.wav")
            safe_output_file = to_short_path(output_file)
            abs_ffmpeg = os.path.abspath(os.path.join(transcriber_dir, 'ffmpeg.exe'))
            abs_ffmpeg_safe = to_short_path(abs_ffmpeg)
            convert_cmd = [
                abs_ffmpeg_safe,
                '-y',
                '-i', safe_temp_file,
                '-ar', '16000',
                '-ac', '1',
                safe_output_file,
            ]
            logger.info(f"üîß FFmpeg –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è ‚Üí WAV: {' '.join(convert_cmd)}")
            import subprocess
            conv = subprocess.run(convert_cmd, capture_output=True, text=True, timeout=30)
            if conv.returncode != 0 or not os.path.exists(output_file) or os.path.getsize(output_file) == 0:
                logger.error(f"‚ùå FFmpeg –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: code={conv.returncode}")
                if conv.stderr:
                    logger.error(f"FFmpeg stderr: {conv.stderr}")
                raise RuntimeError("FFmpeg conversion failed")
            logger.info(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ WAV: {output_file}")

            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ WAV (—à—É–º–æ–¥–∞–≤/VAD), –∑–∞—Ç–µ–º –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ ASR –¥–≤–∏–∂–æ–∫
            async def transcribe_sync() -> Dict[str, Any]:
                try:
                    # VAD/—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
                    final_wav = preprocess_wav(output_file)
                    if engine == 'openai':
                        with open(final_wav, 'rb') as f:
                            loop = asyncio.get_running_loop()
                            response = await loop.run_in_executor(
                                None,
                                lambda: openai.Audio.transcribe(
                                    model="whisper-1",
                                    file=f,
                                    language=language,
                                    temperature=0,
                                    prompt=prompt or DEFAULT_OPENAI_PROMPT,
                                ),
                            )
                            text = getattr(response, "text", None) or response.get("text", "")
                            text = clean_transcript_text(text)
                            return {"text": text, "language": language}
                    elif engine == 'local':
                        asr = get_local_asr()
                        # –ß–∏—Ç–∞–µ–º WAV –≤ numpy, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç –≤–Ω–µ—à–Ω–µ–≥–æ ffmpeg –≤ PATH
                        audio_array, sr = sf.read(final_wav, dtype='float32', always_2d=False)
                        if isinstance(audio_array, tuple):
                            audio_array = audio_array[0]
                        loop = asyncio.get_running_loop()
                        lang = map_language_for_whisper(language)
                        result = await loop.run_in_executor(
                            None,
                            lambda: asr({"array": audio_array, "sampling_rate": sr}, return_timestamps=False, generate_kwargs={"language": lang})
                        )
                        text = result.get('text', '') if isinstance(result, dict) else str(result)
                        text = clean_transcript_text(text)
                        return {"text": text, "language": language}
                    else:
                        return {"error": f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–≤–∏–∂–æ–∫: {engine}"}
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ OpenAI: {e}")
                    return {"error": f"OpenAI –æ—à–∏–±–∫–∞: {e}"}

            return await transcribe_sync()
            
            
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_extension.upper()} —á–µ—Ä–µ–∑ pydub: {e}")
            logger.error(f"üîç –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
            
            # –ü–∞–¥–µ–Ω–∏–µ pydub/–∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ ‚Üí –ø—Ä–æ–±—É–µ–º –µ—â—ë —Ä–∞–∑ –ø—Ä—è–º–æ–π FFmpeg –≤ WAV
            try:
                output_file = os.path.join(temp_dir, f"openai_audio_{int(time.time() * 1000)}.wav")
                safe_output_file = to_short_path(output_file)
                abs_ffmpeg = os.path.abspath(os.path.join(transcriber_dir, 'ffmpeg.exe'))
                abs_ffmpeg_safe = to_short_path(abs_ffmpeg)
                import subprocess
                conv2 = subprocess.run([
                    abs_ffmpeg_safe, '-y', '-i', safe_temp_file, '-ar', '16000', '-ac', '1', safe_output_file
                ], capture_output=True, text=True, timeout=30)
                if conv2.returncode == 0 and os.path.exists(output_file) and os.path.getsize(output_file) > 0:
                    logger.info(f"‚úÖ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ WAV —É—Å–ø–µ—à–Ω–∞: {output_file}")

                    async def transcribe_sync2() -> Dict[str, Any]:
                        try:
                            final_wav = preprocess_wav(output_file)
                            if engine == 'openai':
                                with open(final_wav, 'rb') as f:
                                    loop = asyncio.get_running_loop()
                                    response = await loop.run_in_executor(
                                        None,
                                        lambda: openai.Audio.transcribe(
                                            model="whisper-1",
                                            file=f,
                                            language=language,
                                            temperature=0,
                                            prompt=prompt or DEFAULT_OPENAI_PROMPT,
                                        ),
                                    )
                                    text = getattr(response, "text", None) or response.get("text", "")
                                    text = clean_transcript_text(text)
                                    return {"text": text, "language": language}
                            elif engine == 'local':
                                asr = get_local_asr()
                                audio_array, sr = sf.read(final_wav, dtype='float32', always_2d=False)
                                if isinstance(audio_array, tuple):
                                    audio_array = audio_array[0]
                                loop = asyncio.get_running_loop()
                                lang = map_language_for_whisper(language)
                                result = await loop.run_in_executor(
                                    None,
                                    lambda: asr({"array": audio_array, "sampling_rate": sr}, return_timestamps=False, generate_kwargs={"language": lang})
                                )
                                text = result.get('text', '') if isinstance(result, dict) else str(result)
                                text = clean_transcript_text(text)
                                return {"text": text, "language": language}
                            else:
                                return {"error": f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–≤–∏–∂–æ–∫: {engine}"}
                        except Exception as e:
                            logger.error(f"‚ùå –û—à–∏–±–∫–∞ OpenAI: {e}")
                            return {"error": f"OpenAI –æ—à–∏–±–∫–∞: {e}"}

                    return await transcribe_sync2()
                else:
                    logger.error(f"‚ùå –ü–æ–≤—Ç–æ—Ä–Ω–∞—è FFmpeg –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: code={conv2.returncode}")
                    if conv2.stderr:
                        logger.error(f"FFmpeg stderr: {conv2.stderr}")
                    return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ –≤ WAV"}
            except Exception as fallback_error:
                logger.error(f"‚ùå –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —á–µ—Ä–µ–∑ FFmpeg –Ω–µ —É–¥–∞–ª–∞—Å—å: {fallback_error}")
                return {"error": f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {fallback_error}"}
        
    except Exception as e:
        logger.error(f"‚ùå === –û–ë–©–ê–Ø –û–®–ò–ë–ö–ê –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–ò ===")
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}")
        logger.error(f"üîç –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
        logger.error(f"üìç –ú–µ—Å—Ç–æ –æ—à–∏–±–∫–∏: transcribe_audio_chunk")
        return {"error": f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}"}
    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logger.info("üßπ === –û–ß–ò–°–¢–ö–ê –í–†–ï–ú–ï–ù–ù–´–• –§–ê–ô–õ–û–í ===")
        
        files_to_clean = []
        if 'temp_file' in locals() and os.path.exists(temp_file):
            files_to_clean.append(temp_file)
        if 'output_file' in locals() and os.path.exists(output_file):
            files_to_clean.append(output_file)
        # –£–¥–∞–ª–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã VAD
        try:
            for name in os.listdir(temp_dir):
                if name.endswith('_vad.wav'):
                    candidate = os.path.join(temp_dir, name)
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ VAD-—Ñ–∞–π–ª—ã
                    try:
                        files_to_clean.append(candidate)
                    except Exception:
                        pass
        except Exception:
            pass
        
        for file_path in files_to_clean:
            try:
                os.unlink(file_path)
                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {file_path}")
            except Exception as cleanup_error:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {file_path}: {cleanup_error}")
        
        if not files_to_clean:
            logger.info("‚ÑπÔ∏è –ù–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")

# Startup initialization —É–±—Ä–∞–Ω - –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–µ—Ä—å –≤ main()

@app.get("/")
async def root():
    return {"message": "–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ (–ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º)"}

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    logger.info("üîó –ù–æ–≤–æ–µ WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    try:
        while True:
            message = await websocket.receive()
            if 'text' not in message:
                continue
            try:
                data = json.loads(message['text'])
                message_type = data.get('type')
                logger.info(f"üì® –ü–æ–ª—É—á–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Ç–∏–ø–∞: {message_type}")

                if message_type == 'start':
                    engine = data.get('engine', 'openai')
                    prompt = data.get('prompt')
                    connection_engine[id(websocket)] = json.dumps({"engine": engine, "prompt": prompt})
                    await websocket.send_json({"status": "started", "engine": engine})

                elif message_type == 'stop':
                    await websocket.send_json({"status": "stopped"})

                elif message_type == 'audio':
                    audio_base64 = data.get('data', '')
                    audio_format = data.get('format', 'audio/webm')
                    language = data.get('language', 'ru')
                    engine_info = connection_engine.get(id(websocket))
                    engine = 'openai'
                    prompt = None
                    if engine_info:
                        try:
                            info = json.loads(engine_info)
                            engine = info.get('engine', 'openai')
                            prompt = info.get('prompt')
                        except Exception:
                            pass
                    logger.info(f"üéµ –ü–æ–ª—É—á–µ–Ω—ã base64 –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ: {len(audio_base64)} —Å–∏–º–≤–æ–ª–æ–≤, —Ñ–æ—Ä–º–∞—Ç: {audio_format}")
                    try:
                        audio_data = base64.b64decode(audio_base64)
                        logger.info(f"‚úÖ –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–ª–∏ {len(audio_data)} –±–∞–π—Ç")
                        if len(audio_data) < MIN_CHUNK_BYTES:
                            logger.info("‚è≥ –ß–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –º–∞–ª ‚Äî –∂–¥—ë–º —Å–ª–µ–¥—É—é—â–∏–π")
                            continue
                        result = await transcribe_audio_chunk(audio_data, audio_format, language, engine=engine, prompt=prompt)
                        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {list(result.keys())}")
                        await websocket.send_json(result)
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è base64: {e}")
                        await websocket.send_json({"error": f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}"})
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Å–æ–æ–±—â–µ–Ω–∏—è: {message_type}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
    except WebSocketDisconnect:
        logger.info("üîå WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–∞–∑–æ—Ä–≤–∞–Ω–æ")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ WebSocket –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
    finally:
        try:
            connection_engine.pop(id(websocket), None)
        except Exception:
            pass

if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    # transcription_manager = TranscriptionManager() # This line is removed
    logger.info("üöÄ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ç–æ—Ä –∑–∞–ø—É—â–µ–Ω –≤ –ø—Ä–æ—Å—Ç–æ–º —Ä–µ–∂–∏–º–µ (–±–µ–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏)")
    logger.info("üåê –°–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://localhost:8001")
    logger.info("üîó WebSocket endpoint: ws://localhost:8001/ws")
    uvicorn.run(app, host="0.0.0.0", port=8001) 